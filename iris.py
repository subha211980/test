# -*- coding: utf-8 -*-
"""iris.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17Nn6iA9Ga921Na0v0_4x6sZRP1heXsF9
"""

import networkx as nx
import random
import matplotlib.pyplot as plt

import networkx as nx
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import linregress
import random

def load_graph(file_path):
    ext = file_path.split('.')[-1].lower()
    if ext == 'gml':
        return nx.read_gml(file_path)
    elif ext == 'csv':
        return nx.read_edgelist(file_path, delimiter=',', nodetype=int, create_using=nx.Graph())
    elif ext == 'txt':
        return nx.read_edgelist(file_path, nodetype=int, create_using=nx.Graph())
    else:
        raise ValueError("Unsupported file format. Use txt, csv, or gml.")

def compute_properties(G, graph_type):
    degree_list = []
    for n, d in G.degree():
        degree_list.append(d)
    degrees = np.array(degree_list)

    avg_degree = np.mean(degrees)
    if nx.is_connected(G):
        avg_path = nx.average_shortest_path_length(G)
    else:
        avg_path = float('inf')
        print(f"{graph_type} graph is not connected.")
    clustering = nx.average_clustering(G)
    avg_k2 = np.mean(degrees ** 2)
    kappa = avg_k2 / avg_degree - 1 if avg_degree > 0 else 0
    critical_threshold = 1 / kappa if kappa > 0 else 0

    print(f"{graph_type} - Average Degree: {avg_degree}")
    print(f"{graph_type} - Average Path Length: {avg_path}")
    print(f"{graph_type} - Clustering Coefficient: {clustering}")
    print(f"{graph_type} - Critical Threshold (percolation p_c): {critical_threshold}")

    # Degree distribution plot (log-log)
    unique, counts = np.unique(degrees, return_counts=True)
    p_k = counts / float(len(degrees))
    plt.figure()
    plt.plot(unique, p_k, 'o', label='Data')
    plt.xscale('log')
    plt.yscale('log')
    plt.title(f'Degree Distribution ({graph_type})')
    plt.xlabel('Degree k')
    plt.ylabel('P(k)')
    plt.legend()
    plt.show()

    # For scale-free: check power-law fit and find exponent
    if graph_type == 'Scale-Free':
        mask = (unique >= 2) & (p_k > 0)
        if np.sum(mask) > 1:
            log_k = np.log(unique[mask])
            log_p = np.log(p_k[mask])
            result = linregress(log_k, log_p)
            gamma = -result.slope
            r2 = result.rvalue ** 2
            print(f"{graph_type} - Power-law exponent gamma: {gamma}")
            print(f"{graph_type} - R^2 for power-law fit: {r2}")
            if r2 > 0.9:
                print(f"{graph_type} - Fits power-law distribution well.")
            else:
                print(f"{graph_type} - Does not fit power-law distribution well.")
        else:
            print(f"{graph_type} - Insufficient data for power-law fit.")

    # Check path length between 2 random nodes
    nodes = list(G.nodes)
    u = random.choice(nodes)
    v = random.choice(nodes)
    while u == v:
        v = random.choice(nodes)
    if nx.has_path(G, u, v):
        dist = nx.shortest_path_length(G, u, v)
        print(f"{graph_type} - Distance between random nodes {u} and {v}: {dist}")
        if dist < 6:
            print(f"{graph_type} - Path length < 6 degrees of separation.")
        else:
            print(f"{graph_type} - Path length >= 6 degrees of separation.")
    else:
        print(f"{graph_type} - No path between random nodes {u} and {v}.")

try:
    print("FROM GML reading")
    G_loaded = load_graph('/content/network.gml')
    print("Loaded Graph from File")
    compute_properties(G_loaded, 'Loaded')
except FileNotFoundError:
    print("File not found. Please update the file path to your GML file.")
except Exception as e:
    print(f"An error occurred: {e}")

def generate_powerlaw_configuration(n, gamma, ensure_simple=True):
    seq = nx.utils.powerlaw_sequence(n, gamma)
    seq = [max(1, int(round(x))) for x in seq]
    if sum(seq) % 2 == 1:
        seq[np.random.randint(0, n)] += 1
    G_multi = nx.configuration_model(seq, create_using=None)
    if ensure_simple:
        G = nx.Graph(G_multi)  # removes parallel edges, keeps self-loops as edges (we remove them)
        G.remove_edges_from(nx.selfloop_edges(G))
        return G
    else:
        return G_multi

# Generate and analyze the 3 model graphs
n = 1000  # Number of nodes

# Random (Erdos-Renyi) graph, target avg degree ~10
avg_k = 4 # target average degree
p = avg_k / (n - 1)  # for ER
G_er = nx.erdos_renyi_graph(n, p)
print("\nRandom Graph")
compute_properties(G_er, 'Random')

# Scale-Free (Barabasi-Albert) graph, target avg degree ~10
m_ba = 5  # m=5 gives avg degree ~10
G_ba = nx.barabasi_albert_graph(n, m_ba)
print("\nScale-Free Graph")
compute_properties(G_ba, 'Scale-Free')

# Watts-Strogatz graph, target avg degree 10
k_ws = 10
p_ws = 0.1  # Rewiring probability
G_ws = nx.watts_strogatz_graph(n, k_ws, p_ws)
print("\nWatts-Strogatz Graph")
compute_properties(G_ws, 'Watts-Strogatz')

G_pow=generate_powerlaw_configuration(n, gamma=2.1, ensure_simple=True)
compute_properties(G_pow,'Power law dist')

# Example data (replace this with your actual edge list)
your_edge_list = [
    (1, 2), (1, 3), (1, 4), (2, 3), (2, 5), (3, 6),
    (4, 7), (5, 8), (6, 9), (7, 10), (8, 11), (9, 12),
    (1, 13), (1, 14), (13, 14)
]

# Create a graph from your data
G_real = nx.Graph()
G_real.add_edges_from(your_edge_list)

#Reading from txt file:
# Format: Node1 Node2
# A B
# A C
# B C
# B D
# E F

G_from_file = nx.read_edgelist('edges.txt')
nx.draw(G_from_file, with_labels=True, node_color='lightblue', font_weight='bold')
plt.show()

# Read the graph from the GML file
# Note: For integer node IDs, you might need label='id'
G_from_gml = nx.read_gml('network.gml', label='id')
print("Graph loaded from  successfully!")
print("Nodes:", G_from_gml.nodes())
print("Edges:", G_from_gml.edges())

# Draw the graph
nx.draw(G_from_gml, with_labels=True, node_color='lightgreen', font_weight='bold')
plt.show()

import networkx as nx
import matplotlib.pyplot as plt

# --- 1. Define Common and Model-Specific Parameters ---
# Common parameter for all models
num_nodes = 100

# ER (Erdős-Rényi) parameters
p_er = 0.06  # Probability of edge creation

# WS (Watts-Strogatz) parameters
k_ws = 4     # Number of nearest neighbors to connect to
p_ws = 0.1   # Probability of rewiring

# SF (Scale-Free / Barabási-Albert) parameters
m_sf = 3     # Number of edges for a new node to attach

# --- 2. Generate All Three Graphs ---
G_er = nx.erdos_renyi_graph(num_nodes, p_er)
G_ws = nx.watts_strogatz_graph(num_nodes, k_ws, p_ws)
G_sf = nx.barabasi_albert_graph(num_nodes, m_sf)

# --- 3. Set up the Plotting Area ---
# Create a figure with 1 row and 3 columns of subplots
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# --- 4. Draw Each Network in its Subplot ---

# Erdős-Rényi (Random)
ax0 = axes[0]
nx.draw(G_er, ax=ax0, node_size=50, node_color='coral', edge_color='gray')
ax0.set_title(f'Erdős-Rényi (Random)\nn={num_nodes}, p={p_er}')

# Watts-Strogatz (Small-World)
ax1 = axes[1]
# Use a circular layout to highlight its structure
pos_ws = nx.circular_layout(G_ws)
nx.draw(G_ws, pos=pos_ws, ax=ax1, node_size=50, node_color='violet', edge_color='gray')
ax1.set_title(f'Watts-Strogatz (Small-World)\nn={num_nodes}, k={k_ws}, p={p_ws}')

# Scale-Free (Barabási-Albert)
ax2 = axes[2]
nx.draw(G_sf, ax=ax2, node_size=50, node_color='skyblue', edge_color='gray')
ax2.set_title(f'Scale-Free (Hub-Dominated)\nn={num_nodes}, m={m_sf}')

# Display the plots
fig.suptitle('Comparison of Fundamental Network Models', fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust layout to make room for suptitle
plt.show()

def plot_degree_distribution(G, ax, color, title):
    """Calculates and plots the degree distribution of a graph on given axes."""
    degree_sequence = sorted([d for n, d in G.degree()], reverse=True)
    degree_count = collections.Counter(degree_sequence)
    deg, cnt = zip(*degree_count.items())

    ax.loglog(deg, cnt, 'o', color=color) # Use loglog for both axes
    ax.set_title(title)
    ax.set_xlabel("Degree (k)")
    ax.set_ylabel("Count P(k)")
    ax.grid(True, which="both", ls="--", linewidth=0.5)

fig, axes = plt.subplots(1, 3, figsize=(20, 6))
fig.suptitle('Degree Distribution Comparison on Log-Log Axes', fontsize=16)

plot_degree_distribution(G_er, axes[0], 'coral', 'Erdős-Rényi')
plot_degree_distribution(G_ws, axes[1], 'violet', 'Watts-Strogatz')
plot_degree_distribution(G_sf, axes[2], 'skyblue', 'Scale-Free')

plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()

import networkx as nx
import matplotlib.pyplot as plt
import numpy as np

# Let's use a larger BA graph for better analysis
G = nx.barabasi_albert_graph(1000, 3) # 1000 nodes, each new node attaches to 3 existing ones

# --- A. Degree and Degree Distribution ---
# The degree of a node is its number of connections.
degrees = [G.degree(n) for n in G.nodes()]

print(f"Number of nodes: {G.number_of_nodes()}")
print(f"Number of edges: {G.number_of_edges()}")
print(f"Average degree: {np.mean(degrees):.2f}")

# --- B. Average Path Length & Six Degrees of Separation ---
# This concept means that most people are just a few "friend of a friend" steps away from each other.
# In graph theory, it's measured by the average shortest path length.
# NOTE: This can be slow for very large graphs and only works on a connected graph.
if nx.is_connected(G):
    avg_path_len = nx.average_shortest_path_length(G)
    print(f"Average shortest path length: {avg_path_len:.2f}")
else:
    # If the graph is not connected, we can analyze the largest connected component
    largest_cc = max(nx.connected_components(G), key=len)
    G_largest_cc = G.subgraph(largest_cc)
    avg_path_len = nx.average_shortest_path_length(G_largest_cc)
    print(f"Graph is not connected. Average shortest path length of largest component: {avg_path_len:.2f}")


# --- C. Clustering Coefficient ---
# Measures how close a node's neighbors are to being a complete graph (a clique).
# A high value means nodes tend to form tight-knit groups.
avg_clustering = nx.average_clustering(G)
print(f"Average clustering coefficient: {avg_clustering:.2f}")

# You can also get the coefficient for a specific node:
# clustering_node_0 = nx.clustering(G, 0)


# --- D. Degree Distribution Plot ---
# This plot shows how many nodes have each degree.
degree_sequence = sorted([d for n, d in G.degree()], reverse=True)
degree_counts = nx.degree_histogram(G)
degrees_x = range(len(degree_counts))

plt.figure(figsize=(12, 5))

# Plot 1: Standard linear plot
plt.subplot(1, 2, 1)
plt.bar(degrees_x, degree_counts, width=0.80, color="b")
plt.title("Degree Distribution")
plt.xlabel("Degree")
plt.ylabel("Number of Nodes")

# Plot 2: Log-log plot to check for Power Law
# A straight line on a log-log plot suggests a power-law distribution.
plt.subplot(1, 2, 2)
plt.loglog(degrees_x, degree_counts, 'bo')
plt.title("Degree Distribution (Log-Log Scale)")
plt.xlabel("Degree (log)")
plt.ylabel("Number of Nodes (log)")

plt.tight_layout()
plt.show()

import numpy as np

# We'll use the same graph G from the previous section
degrees = [d for n, d in G.degree()]

# <k> is the average degree
k_avg = np.mean(degrees)

# <k^2> is the average of the squared degrees
k2_avg = np.mean([d**2 for d in degrees])

if k_avg > 0:
    critical_threshold = k2_avg / k_avg
    print(f"Average degree <k>: {k_avg:.2f}")
    print(f"Average squared degree <k^2>: {k2_avg:.2f}")
    print(f"Critical Threshold (<k^2>/<k>): {critical_threshold:.2f}")
else:
    print("Graph has no edges; cannot calculate threshold.")

import pandas as pd
import networkx as nx
import collections
import matplotlib.pyplot as plt

# --- Step 1: Create a Dummy CSV File for Demonstration ---
# In a real scenario, you would already have this file.
csv_data = {
    'source': [1, 1, 1, 2, 2, 3, 4, 5, 6, 7, 8, 9, 1, 1, 13, 15, 15],
    'target': [2, 3, 4, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 14, 16, 1]
}
df_for_csv = pd.DataFrame(csv_data)
df_for_csv.to_csv('network_data.csv', index=False)
print("Created a sample 'network_data.csv' file.")

# --- Step 2: Read the CSV and Build the Graph ---
# Use pandas to read the CSV into a DataFrame
try:
    edge_df = pd.read_csv('network_data.csv')

    # Use networkx to create a graph directly from the pandas DataFrame
    # It assumes the columns for source and target nodes are named 'source' and 'target'
    G_real = nx.from_pandas_edgelist(edge_df, 'source', 'target')
    print("\nSuccessfully created graph from CSV.")

except FileNotFoundError:
    print("Error: 'network_data.csv' not found. Please create it or check the path.")
    exit()

# --- Step 3: Measure Key Properties of Your Graph ---
N = G_real.number_of_nodes()
L = G_real.number_of_edges()
avg_degree = (2 * L) / N
avg_clustering = nx.average_clustering(G_real)

print(f"\n--- Real Graph Properties ---")
print(f"Nodes (N): {N}")
print(f"Edges (L): {L}")
print(f"Average Degree <k>: {avg_degree:.4f}")
print(f"Average Clustering Coefficient C: {avg_clustering:.4f}")

# --- Step 4: Estimate Parameters for Each Model ---
print("\n--- Estimating Model Parameters ---")

# ER Parameters
N_er = N
p_er = avg_degree / (N - 1)
print(f"Erdős-Rényi: N={N_er}, p={p_er:.4f}")

# WS Parameters
N_ws = N
k_ws = round(avg_degree)
if k_ws % 2 != 0: k_ws += 1 # k must be even
print(f"Watts-Strogatz: N={N_ws}, k={k_ws}, p=(requires fitting)")

# SF/BA Parameters
N_sf = N
m_sf = round(avg_degree / 2)
print(f"Scale-Free (BA): N={N_sf}, m={m_sf}")

# --- Step 5: Plot the Real Degree Distribution to Check for Best Fit ---
degree_sequence = sorted([d for n, d in G_real.degree()], reverse=True)
degree_count = collections.Counter(degree_sequence)
deg, cnt = zip(*degree_count.items())

plt.figure(figsize=(8, 6))
plt.loglog(deg, cnt, 'o', color='navy')
plt.title("Degree Distribution of Data from CSV (Log-Log Scale)")
plt.xlabel("Degree (k)")
plt.ylabel("Count P(k)")
plt.grid(True)
plt.show()

"""Step 4: How to Know Which Model Fits Best?
Just calculating parameters isn't enough. You need to determine which model's structure best resembles your data. The best way is to compare the degree distributions.

Plot your real data's degree distribution on a log-log scale.

Check its shape:

If it forms a straight line, your network is likely Scale-Free. The BA model is your best choice.

If it has a clear peak and curves down sharply (like a bell curve), it's either ER or WS.

If it's peaked, check the clustering coefficient:

Generate an ER graph with your estimated parameters (N_er, p_er).

Calculate its average clustering coefficient.

If your real graph's clustering coefficient (avg_clustering) is much higher than the ER model's, then the Watts-Strogatz model is a better fit because it's specifically designed to have high clustering. Otherwise, ER is a reasonable approximation.

"""

def erdos_renyi_graph(N, p):
    G = nx.Graph()
    G.add_nodes_from(range(N))
    for i in range(N):
        for j in range(i + 1, N):
            if random.random() < p:
                G.add_edge(i, j)
    return G
N = 20
p = 0.1
G = erdos_renyi_graph(N, p)

plt.figure(figsize=(6, 6))
nx.draw(G, node_size=200, with_labels=True)
plt.title(f"Erdős–Rényi G({N}, {p}) Random Graph")
plt.show()

"""# Track Larget component ER"""

import networkx as nx
import matplotlib.pyplot as plt
import random

# Parameters
N = 500       # number of nodes
avg_k = 4     # average degree
p = avg_k / (N - 1)

# Generate ER network
G = nx.erdos_renyi_graph(N, p)

# Track largest component size
largest_component_sizes = []
fraction_removed = []

# Create a list of nodes to remove
nodes = list(G.nodes())
random.shuffle(nodes)

for i, node in enumerate(nodes):
    # Remove the node
    G.remove_node(node)

    # Compute size of largest connected component
    if len(G) > 0:
        largest_cc = max(nx.connected_components(G), key=len)
        largest_component_sizes.append(len(largest_cc))
    else:
        largest_component_sizes.append(0)

    fraction_removed.append((i + 1) / N)

# Plot the results
plt.figure(figsize=(8,5))
plt.plot(fraction_removed, largest_component_sizes, marker='o', linestyle='-', color='red')
plt.xlabel("Fraction of nodes removed")
plt.ylabel("Size of largest connected component")
plt.title("Robustness of ER Network under Random Node Failures")
plt.grid(True)
plt.show()

"""#  All metrics"""

import networkx as nx
import matplotlib.pyplot as plt
import collections
from networkx.algorithms import community

# Load GML file
G = nx.read_gml("network.gml")

# 1. Basic properties
N = G.number_of_nodes()
L = G.number_of_edges()
avg_degree = sum(dict(G.degree()).values()) / N
print(f"Nodes: {N}, Edges: {L}, Avg degree: {avg_degree:.2f}")

# 2. Connectivity
components = sorted(nx.connected_components(G), key=len, reverse=True)
GCC = G.subgraph(components[0])
print(f"Connected components: {len(components)}")
print(f"GCC size: {GCC.number_of_nodes()} nodes")

# 3. Path stats
avg_path = nx.average_shortest_path_length(GCC)
diameter = nx.diameter(GCC)
print(f"Avg path length: {avg_path:.2f}, Diameter: {diameter}")

# 4. Clustering
clustering = nx.average_clustering(G)
print(f"Avg clustering coefficient: {clustering:.3f}")

# 5. Centralities
deg_cent = nx.degree_centrality(G)
bet_cent = nx.betweenness_centrality(G)
close_cent = nx.closeness_centrality(G)
eig_cent = nx.eigenvector_centrality(G)

# Show top nodes by degree
sorted_deg = sorted(deg_cent.items(), key=lambda x: x[1], reverse=True)[:5]
print("Top 5 nodes by degree centrality:", sorted_deg)

# 6. Community detection (Louvain if available, else greedy modularity)
communities = community.greedy_modularity_communities(G)
print(f"Detected {len(communities)} communities")

# 7. Degree distribution
degrees = [d for n, d in G.degree()]
degree_count = collections.Counter(degrees)
deg, cnt = zip(*sorted(degree_count.items()))

plt.figure(figsize=(6,4))
plt.loglog(deg, cnt, "bo")
plt.xlabel("Degree k")
plt.ylabel("Count")
plt.title("Degree distribution (log-log)")
plt.show()

# 8. Network visualization
plt.figure(figsize=(8,8))
pos = nx.spring_layout(G, seed=42)
nx.draw_networkx_nodes(G, pos, node_size=50, cmap=plt.cm.Set1)
nx.draw_networkx_edges(G, pos, alpha=0.3)
plt.title("Network Visualization")
plt.show()

import networkx as nx
import matplotlib.pyplot as plt
import collections
import numpy as np
from networkx.algorithms import community

# Load network
G = nx.read_gml("network.gml")

# Ensure largest connected component for path-based metrics
components = sorted(nx.connected_components(G), key=len, reverse=True)
GCC = G.subgraph(components[0]).copy()

# Degree distribution
degrees = [d for n, d in G.degree()]
deg_count = collections.Counter(degrees)
deg, cnt = zip(*sorted(deg_count.items()))

plt.figure(figsize=(6,4))
plt.bar(deg, cnt)
plt.title("Degree Distribution")
plt.xlabel("Degree k")
plt.ylabel("Count")
plt.show()

plt.figure(figsize=(6,4))
plt.loglog(deg, cnt, "bo")
plt.title("Degree Distribution (Log-Log)")
plt.xlabel("Degree k")
plt.ylabel("Count")
plt.show()

# Cumulative degree distribution
sorted_deg = sorted(degrees)
cdf = np.arange(len(sorted_deg))/len(sorted_deg)
plt.figure(figsize=(6,4))
plt.plot(sorted_deg, 1-cdf)
plt.yscale("log")
plt.xscale("log")
plt.xlabel("Degree k")
plt.ylabel("CCDF")
plt.title("Cumulative Degree Distribution")
plt.show()

# Shortest path length distribution
lengths = dict(nx.all_pairs_shortest_path_length(GCC))
all_lengths = [l for d in lengths.values() for l in d.values()]
plt.hist(all_lengths, bins=30)
plt.title("Shortest Path Length Distribution")
plt.xlabel("Path length")
plt.ylabel("Frequency")
plt.show()

# Clustering coefficient histogram
clust = nx.clustering(G)
plt.hist(clust.values(), bins=20)
plt.title("Local Clustering Coefficients")
plt.xlabel("C")
plt.ylabel("Count")
plt.show()

# Clustering vs degree
plt.scatter(degrees, list(clust.values()), alpha=0.5)
plt.xscale("log")
plt.title("Clustering vs Degree")
plt.xlabel("Degree k")
plt.ylabel("Clustering C")
plt.show()

# Centrality histograms
centralities = {
    "Degree Centrality": nx.degree_centrality(G),
    "Betweenness": nx.betweenness_centrality(G),
    "Closeness": nx.closeness_centrality(G),
    "Eigenvector": nx.eigenvector_centrality(G, max_iter=500),
}

for name, cent in centralities.items():
    plt.hist(cent.values(), bins=30)
    plt.title(f"{name} Distribution")
    plt.show()

# Communities
communities = community.greedy_modularity_communities(G)
sizes = [len(c) for c in communities]
plt.hist(sizes, bins=20)
plt.title("Community Size Distribution")
plt.xlabel("Size")
plt.ylabel("Count")
plt.show()

# Assortativity: average neighbor degree vs node degree
avg_nbr_deg = nx.average_neighbor_degree(G)
plt.scatter(degrees, list(avg_nbr_deg.values()), alpha=0.5)
plt.xscale("log")
plt.yscale("log")
plt.xlabel("Node degree k")
plt.ylabel("Average neighbor degree")
plt.title("Degree-Degree Correlation")
plt.show()

# Network visualizations
plt.figure(figsize=(8,8))
pos = nx.spring_layout(G, seed=42)
nx.draw(G, pos, node_size=30, node_color="blue", edge_color="gray", alpha=0.6)
plt.title("Network Visualization (Spring Layout)")
plt.show()

plt.figure(figsize=(8,8))
nx.draw_circular(G, node_size=30, alpha=0.6)
plt.title("Network Visualization (Circular Layout)")
plt.show()

plt.figure(figsize=(8,8))
nx.draw_shell(G, node_size=30, alpha=0.6)
plt.title("Network Visualization (Shell Layout)")
plt.show()

"""# Read from file"""

import networkx as nx

# Unweighted edge list
G = nx.read_edgelist("network.txt", nodetype=int)

# Weighted edge list
Gw = nx.read_edgelist("network.txt", nodetype=int, data=(("weight", float),))

G = nx.read_adjlist("network.txt", nodetype=int)

#adj matrix
A = np.loadtxt("network.txt", dtype=int)
G = nx.from_numpy_array(A)

G = nx.read_gml("network.txt")

import csv

G = nx.Graph()
with open("network.txt") as f:
    reader = csv.reader(f, delimiter=",")  # change delimiter if needed
    for row in reader:
        u, v = row[0], row[1]
        G.add_edge(u, v)

import networkx as nx
import numpy as np
import matplotlib.pyplot as plt

N = 103
avg_k = 2

# Generate ER graph
p = avg_k / (N - 1)
G_er = nx.erdos_renyi_graph(N, p, seed=42)

# Generate scale-free network using power-law degree sequence
gamma = 3  # exponent for power-law P(k) ~ k^-gamma
min_degree = 1

# Generate power-law degree sequence
degree_sequence = np.random.zipf(gamma, N)
# Ensure all degrees >= min_degree and sum is even
degree_sequence = np.maximum(degree_sequence, min_degree)
if sum(degree_sequence) % 2 != 0:
    degree_sequence[0] += 1

# Generate graph using configuration model
G_sf = nx.configuration_model(degree_sequence, seed=42)
G_sf = nx.Graph(G_sf)  # remove parallel edges
G_sf.remove_edges_from(nx.selfloop_edges(G_sf))

# Plot degree distributions
plt.figure(figsize=(10,5))

# ER degree distribution
degrees_er = [d for n, d in G_er.degree()]
plt.hist(degrees_er, bins=range(max(degrees_er)+2), alpha=0.6, label='ER', color='skyblue')

# Scale-free degree distribution
degrees_sf = [d for n, d in G_sf.degree()]
plt.hist(degrees_sf, bins=range(max(degrees_sf)+2), alpha=0.6, label='Power-law SF', color='orange')

plt.xlabel("Degree k")
plt.ylabel("Frequency")
plt.title("Degree Distributions")
plt.legend()
plt.show()

# ----------------------------
# 1. ER Graph Visualization
# ----------------------------
plt.figure(figsize=(8, 6))
pos_er = nx.spring_layout(G_er, seed=42)
nx.draw_networkx(G_er, pos_er,
                 node_size=50,
                 node_color="skyblue",
                 edge_color="gray",
                 with_labels=False)
plt.title("Erdős–Rényi (ER) Graph")
plt.show()

# ----------------------------
# 2. Scale-Free Graph Visualization
# ----------------------------
plt.figure(figsize=(8, 6))
pos_sf = nx.spring_layout(G_sf, seed=42)

# Node degrees for coloring
node_degrees = np.array([d for n, d in G_sf.degree()])

# Draw nodes
nodes = nx.draw_networkx_nodes(G_sf, pos_sf,
                               node_size=50,
                               node_color=node_degrees,
                               cmap=plt.cm.plasma)

# Draw edges
nx.draw_networkx_edges(G_sf, pos_sf, alpha=0.5, edge_color="gray")

# Add colorbar
cbar = plt.colorbar(nodes)
cbar.set_label("Node Degree")

plt.title("Scale-Free (Power-law) Graph with Degree Coloring")
plt.show()

import networkx as nx
import numpy as np

N = 103
avg_k = 2

# ER graph
p_er = avg_k / (N-1)
G_er = nx.erdos_renyi_graph(N, p_er)

# Power-law network
gamma = 3
degree_sequence_pl = np.random.zipf(gamma, N)
degree_sequence_pl = np.maximum(degree_sequence_pl, 1)
if sum(degree_sequence_pl) % 2 != 0:
    degree_sequence_pl[0] += 1
G_pl = nx.configuration_model(degree_sequence_pl)
G_pl = nx.Graph(G_pl)
G_pl.remove_edges_from(nx.selfloop_edges(G_pl))

# Uniform degree network
degree_sequence_uniform = np.random.randint(1, 5, N)  # degrees 1-4
if sum(degree_sequence_uniform) % 2 != 0:
    degree_sequence_uniform[0] += 1
G_uniform = nx.configuration_model(degree_sequence_uniform)
G_uniform = nx.Graph(G_uniform)
G_uniform.remove_edges_from(nx.selfloop_edges(G_uniform))

# Normal degree network
mean_k = avg_k
std_k = 1
degree_sequence_normal = np.random.normal(mean_k, std_k, N).astype(int)
degree_sequence_normal = np.clip(degree_sequence_normal, 1, N-1)
if sum(degree_sequence_normal) % 2 != 0:
    degree_sequence_normal[0] += 1
G_normal = nx.configuration_model(degree_sequence_normal)
G_normal = nx.Graph(G_normal)
G_normal.remove_edges_from(nx.selfloop_edges(G_normal))

def critical_threshold(G):
    degrees = np.array([d for n, d in G.degree()])
    k_avg = degrees.mean()
    k2_avg = (degrees**2).mean()
    return k_avg / (k2_avg - k_avg)

pc_er = critical_threshold(G_er)
pc_pl = critical_threshold(G_pl)
pc_uniform = critical_threshold(G_uniform)
pc_normal = critical_threshold(G_normal)

print("Critical thresholds:")
print(f"ER network: {pc_er:.4f}")
print(f"Power-law network: {pc_pl:.4f}")
print(f"Uniform degree network: {pc_uniform:.4f}")
print(f"Normal degree network: {pc_normal:.4f}")

"""# Similar to Wattz schtrogatz"""

import networkx as nx
import random
import matplotlib.pyplot as plt
import numpy as np

N = 100
m = 2
p_rewire = 0.1

G = nx.cycle_graph(5)


for new_node in range(5, N):
    G.add_node(new_node)

    # Compute degrees and total degree
    degrees = np.array([G.degree(n) for n in G.nodes()])
    total_deg = degrees.sum()

    # Preferential attachment: select m nodes proportional to degree
    targets = set()
    while len(targets) < m:
        rand_node = np.random.choice(G.nodes(), p=degrees/total_deg)
        if rand_node != new_node:
            targets.add(rand_node)

    # Connect new node
    for t in targets:
        G.add_edge(new_node, t)

        # Step 3: Rewire with probability p_rewire
        if random.random() < p_rewire:
            G.remove_edge(new_node, t)
            possible_nodes = list(set(G.nodes()) - {new_node})
            new_target = random.choice(possible_nodes)
            G.add_edge(new_node, new_target)

# --- Analysis ---
print("Number of nodes:", G.number_of_nodes())
print("Number of edges:", G.number_of_edges())
print("Average clustering coefficient:", nx.average_clustering(G))

# Diameter of largest component
largest_cc_nodes = max(nx.connected_components(G), key=len)
largest_cc = G.subgraph(largest_cc_nodes)
print("Diameter (largest component):", nx.diameter(largest_cc))


# --- Degree Distribution ---
degrees = [d for n, d in G.degree()]
plt.figure(figsize=(8,5))
plt.hist(degrees, bins=range(max(degrees)+1), alpha=0.7, color='orange')
plt.xlabel("Degree k")
plt.ylabel("Number of nodes")
plt.title("Degree Distribution of Hybrid Network")
plt.show()

# --- Visualize network ---
plt.figure(figsize=(8,8))
nx.draw_spring(G, node_size=50, node_color='skyblue', edge_color='gray')
plt.title("Hybrid Network Visualization")
plt.show()

"""# Separate communities"""

import networkx as nx
import random
import numpy as np
import matplotlib.pyplot as plt
import community.community_louvain as community_louvain  # Louvain for community detection

# ----------------------------
# Parameters
# ----------------------------
N = 120       # total nodes
C = 3         # number of communities
n0 = 5        # initial nodes per community
m = 2         # edges per new node
p_inter = 0.05  # probability of inter-community edge

G = nx.Graph()
community_map = {}  # node -> planted community

# ----------------------------
# Step 1: Create seed communities
# ----------------------------
node_id = 0
for c in range(C):
    H = nx.erdos_renyi_graph(n0, 0.5)
    mapping = {n: n + node_id for n in H.nodes()}
    H = nx.relabel_nodes(H, mapping)
    G = nx.compose(G, H)
    for n in H.nodes():
        community_map[n] = c
    node_id += n0

# ----------------------------
# Step 2: Add new nodes with preferential attachment
# ----------------------------
while G.number_of_nodes() < N:
    new_node = node_id
    G.add_node(new_node)

    # Pick a community randomly
    c = random.randint(0, C - 1)
    community_map[new_node] = c  # assign community right away
    node_id += 1

    # Preferential attachment inside community
    community_nodes = [n for n in G.nodes() if community_map[n] == c and n != new_node]
    degrees = np.array([G.degree(n) for n in community_nodes])
    total_deg = degrees.sum()

    targets = set()
    while len(targets) < m:
        if total_deg == 0:
            t = random.choice(community_nodes)
        else:
            t = np.random.choice(community_nodes, p=degrees / total_deg)
        targets.add(t)

    # Possibly rewire some edges to other communities
    for t in list(targets):
        if random.random() < p_inter:
            other_com = random.choice([i for i in range(C) if i != c])
            other_nodes = [n for n in G.nodes() if community_map[n] == other_com]
            t = random.choice(other_nodes)
        G.add_edge(new_node, t)

# ----------------------------
# Analysis
# ----------------------------
print("Number of nodes:", G.number_of_nodes())
print("Number of edges:", G.number_of_edges())
print("Average clustering coefficient:", nx.average_clustering(G))

# ----------------------------
# Louvain community detection
# ----------------------------
partition = community_louvain.best_partition(G)
num_communities = len(set(partition.values()))
modularity = community_louvain.modularity(partition, G)

print("Detected communities:", num_communities)
print("Modularity:", modularity)

# ----------------------------
# Visualization
# ----------------------------
pos = nx.spring_layout(G, seed=42)
colors = [partition[n] for n in G.nodes()]  # Louvain communities
plt.figure(figsize=(8, 8))
nx.draw_networkx(G, pos, node_color=colors, cmap=plt.cm.Set3, node_size=100, with_labels=False)
plt.title("Synthetic Network with Planted Communities & Louvain Detection")
plt.show()

import networkx as nx
import matplotlib.pyplot as plt

N = 1000
k_values = [0.5 + 0.05*i for i in range(20)]  # <k> from 0.5 to 1.45
largest_component_fraction = []

for k_avg in k_values:
    p = k_avg / (N-1)
    G = nx.erdos_renyi_graph(N, p)

    if len(G) > 0:
        largest_cc = max(nx.connected_components(G), key=len)
        largest_component_fraction.append(len(largest_cc)/N)
    else:
        largest_component_fraction.append(0)

# Plot
plt.figure(figsize=(8,5))
plt.plot(k_values, largest_component_fraction, marker='o')
plt.axvline(1, color='red', linestyle='--', label="Critical <k>=1")
plt.xlabel("Average degree <k>")
plt.ylabel("Fraction of nodes in largest component")
plt.title("Phase Transition in ER Networks")
plt.legend()
plt.grid(True)
plt.show()

#PROGRAMMING AASIGNMENT 1

import networkx as nx
import matplotlib.pyplot as plt
import numpy as np

# Task 1: Generate and visualize G(N, p) networks
def generate_and_visualize_erdos_renyi(N, avg_degree, title):
    p = avg_degree / (N - 1)
    G = nx.erdos_renyi_graph(N, p)
    plt.figure(figsize=(8, 8))
    pos = nx.spring_layout(G)
    nx.draw(G, pos, node_size=20, node_color='blue', with_labels=False)
    plt.title(title)
    plt.show()  # Display the plot

# Task 2: Red and Blue network
def generate_red_blue_network(N, p, q):
    G = nx.Graph()
    nodes = list(range(2 * N))
    G.add_nodes_from(nodes)
    colors = {i: 'red' if i < N else 'blue' for i in nodes}
    nx.set_node_attributes(G, colors, 'color')
    for i in range(2 * N):
        for j in range(i + 1, 2 * N):
            if colors[i] == colors[j] and np.random.random() < p:
                G.add_edge(i, j)
            elif colors[i] != colors[j] and np.random.random() < q:
                G.add_edge(i, j)
    return G

def check_connectivity(G):
    return nx.number_connected_components(G)

def average_shortest_path_length(G):
    if nx.is_connected(G):
        return nx.average_shortest_path_length(G)
    return float('inf')

# Task 3: Red, Blue, Purple network
def generate_red_blue_purple_network(N, f, p):
    total_nodes = 2 * N
    num_purple = int(f * total_nodes)
    num_red_blue = total_nodes - num_purple
    N_red = num_red_blue // 2
    N_blue = num_red_blue - N_red
    G = nx.Graph()
    nodes = list(range(total_nodes))
    G.add_nodes_from(nodes)
    colors = {i: 'red' if i < N_red else 'blue' if i < N_red + N_blue else 'purple' for i in nodes}
    nx.set_node_attributes(G, colors, 'color')
    for i in range(total_nodes):
        for j in range(i + 1, total_nodes):
            if colors[i] == colors[j] and colors[i] in ['red', 'blue'] and np.random.random() < p:
                G.add_edge(i, j)
            elif colors[i] == 'purple' and colors[j] in ['red', 'blue'] and np.random.random() < p:
                G.add_edge(i, j)
            elif colors[j] == 'purple' and colors[i] in ['red', 'blue'] and np.random.random() < p:
                G.add_edge(i, j)
    return G

def check_two_step_connectivity(G, N):
    red_nodes = [n for n, attr in G.nodes(data=True) if attr['color'] == 'red']
    blue_nodes = [n for n, attr in G.nodes(data=True) if attr['color'] == 'blue']
    for red in red_nodes[:10]:  # Sample 10 nodes
        for blue in blue_nodes[:10]:
            paths = list(nx.all_simple_paths(G, red, blue, cutoff=2))
            if not any(len(path) == 3 for path in paths):
                return False
    return True

# Main execution
if __name__ == "__main__":
    N = 500

    # Task 1: Generate and display three networks
    avg_degrees = [0.8, 1, 8]
    for k in avg_degrees:
        title = f'Erdős-Rényi Network (N={N}, <k>={k})'
        generate_and_visualize_erdos_renyi(N, k, title)

    # Task 2b: Check connectivity
    p = 1 / (N - 1)
    q = 1 / N
    G_rb = generate_red_blue_network(N, p, q)
    num_components = check_connectivity(G_rb)
    print(f"Task 2b: Number of components with p={p:.6f}, q={q:.6f}: {num_components}")

    # Task 2c: Small-world property
    p_snobbish = 0.01
    q_snobbish = 0.0001
    G_snobbish = generate_red_blue_network(N, p_snobbish, q_snobbish)
    if nx.is_connected(G_snobbish):
        avg_path = average_shortest_path_length(G_snobbish)
        print(f"Task 2c: Average shortest path length (p={p_snobbish}, q={q_snobbish}): {avg_path:.2f}")
    else:
        print("Task 2c: Network is not connected")

    # Task 3a: Test two-step connectivity
    p = 8 / (N - 1)
    f_values = [0.01, 0.05, 0.1]
    for f in f_values:
        G_rbp = generate_red_blue_purple_network(N, f, p)
        is_interactive = check_two_step_connectivity(G_rbp, N)
        print(f"Task 3a: f={f}, Interactive: {is_interactive}")

#PROGRAMMING AASIGNMENT 2
# BA model not there so dont study that
import networkx as nx
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from collections import Counter

# Task 1: Power-law degree distribution networks
def generate_power_law_sequence(N, gamma):
    # Generate power-law degree sequence
    degrees = np.random.pareto(gamma - 1, N).astype(int) + 1  # Ensure minimum degree 1
    # Adjust to make sum even
    if sum(degrees) % 2 != 0:
        degrees[np.random.randint(0, N)] += 1
    return degrees

def compute_multi_self_loops(N, gamma, trials=5):
    self_loop_percent = []
    multi_link_percent = []
    for _ in range(trials):
        # Generate configuration model
        degrees = generate_power_law_sequence(N, gamma)
        G_multi = nx.configuration_model(degrees)
        total_edges = G_multi.number_of_edges()

        # Count self-loops
        self_loops = sum(1 for u, v in G_multi.edges() if u == v)
        self_loop_percent.append((self_loops / total_edges) * 100 if total_edges > 0 else 0)

        # Convert to simple graph to count multi-links
        G_simple = nx.Graph(G_multi)
        multi_links = total_edges - G_simple.number_of_edges() - self_loops
        multi_link_percent.append((multi_links / total_edges) * 100 if total_edges > 0 else 0)

    return np.mean(self_loop_percent), np.mean(multi_link_percent)

def plot_percentages(N_values, gamma_values):
    for gamma in gamma_values:
        self_loops = []
        multi_links = []
        for N in N_values:
            sl, ml = compute_multi_self_loops(N, gamma)
            self_loops.append(sl)
            multi_links.append(ml)

        plt.figure(figsize=(8, 6))
        plt.plot(N_values, self_loops, label='Self-loops', marker='o')
        plt.plot(N_values, multi_links, label='Multi-links', marker='s')
        plt.xscale('log')
        plt.xlabel('Number of Nodes (N)')
        plt.ylabel('Percentage (%)')
        plt.title(f'Multi-links and Self-loops for γ = {gamma}')
        plt.legend()
        plt.grid(True, which="both", ls="--")
        plt.show()

# Task 2: Barabási-Albert network
def generate_barabasi_albert(N, m):
    # Start with a fully connected graph of m nodes
    G = nx.complete_graph(m)
    degrees = [m-1] * m  # Initial degrees
    snapshots = []
    target_N = [100, 1000, 10000]

    for new_node in range(m, N):
        # Preferential attachment: choose m nodes
        probs = np.array(degrees) / sum(degrees)
        targets = np.random.choice(list(G.nodes()), size=m, replace=False, p=probs)
        G.add_node(new_node)
        for target in targets:
            G.add_edge(new_node, target)
            degrees[target] += 1
        degrees.append(m)

        # Snapshot at target sizes
        if new_node + 1 in target_N:
            snapshots.append(G.copy())

    return snapshots

def fit_power_law(degree_counts, k_min=1):
    degrees = []
    for k, count in degree_counts.items():
        degrees.extend([k] * count)
    degrees = np.array(degrees)
    degrees = degrees[degrees >= k_min]  # Filter small degrees
    if len(degrees) == 0:
        return None, None
    log_degrees = np.log(degrees)
    gamma = 1 + len(degrees) / np.sum(log_degrees - np.log(k_min))
    return gamma, len(degrees)

def plot_degree_distributions(snapshots, target_N):
    plt.figure(figsize=(8, 6))
    for i, G in enumerate(snapshots):
        degree_counts = Counter(dict(G.degree()).values())
        degrees = np.array(list(degree_counts.keys()))
        counts = np.array(list(degree_counts.values()))
        probs = counts / sum(counts)
        plt.loglog(degrees, probs, 'o', label=f'N={target_N[i]}')

        # Fit power-law
        gamma, n = fit_power_law(degree_counts)
        if gamma:
            k = np.array(list(degree_counts.keys()))
            fitted = k ** (-gamma) / np.sum(k ** (-gamma))
            plt.loglog(k, fitted, '--', label=f'Fit γ={gamma:.2f}')

    plt.xlabel('Degree (k)')
    plt.ylabel('P(k)')
    plt.title('Degree Distributions')
    plt.legend()
    plt.grid(True, which="both", ls="--")
    plt.show()

def plot_cumulative_distributions(snapshots, target_N):
    plt.figure(figsize=(8, 6))
    for i, G in enumerate(snapshots):
        degree_counts = Counter(dict(G.degree()).values())
        degrees = np.array(list(degree_counts.keys()))
        counts = np.array(list(degree_counts.values()))
        sorted_degrees = np.sort(degrees)
        cum_probs = np.cumsum(counts[np.argsort(degrees)][::-1]) / sum(counts)
        plt.loglog(sorted_degrees, cum_probs, 'o-', label=f'N={target_N[i]}')

    plt.xlabel('Degree (k)')
    plt.ylabel('P(K ≥ k)')
    plt.title('Cumulative Degree Distributions')
    plt.legend()
    plt.grid(True, which="both", ls="--")
    plt.show()

def compute_clustering_coefficient(N_values, m):
    clustering = []
    for N in N_values:
        G = nx.barabasi_albert_graph(N, m)
        clustering.append(nx.average_clustering(G))

    plt.figure(figsize=(8, 6))
    plt.plot(N_values, clustering, marker='o')
    plt.xscale('log')
    plt.xlabel('Number of Nodes (N)')
    plt.ylabel('Average Clustering Coefficient')
    plt.title('Clustering Coefficient vs N (m=4)')
    plt.grid(True, which="both", ls="--")
    plt.show()

# Main execution
if __name__ == "__main__":
    # Task 1: Power-law networks
    N_values = [100, 500, 1000, 5000, 10000, 50000, 100000]
    gamma_values = [2.2, 3.0]
    for gamma in gamma_values:
        print(f"\nTask 1: γ = {gamma}")
        for N in [1000, 10000, 100000]:
            sl, ml = compute_multi_self_loops(N, gamma)
            print(f"N={N}: Self-loops = {sl:.2f}%, Multi-links = {ml:.2f}%")
    plot_percentages(N_values, gamma_values)

    # Task 2: Barabási-Albert network
    N = 10000
    m = 4
    target_N = [100, 1000, 10000]
    snapshots = generate_barabasi_albert(N, m)

    # Task 2a, 2b: Degree distributions and power-law fit
    plot_degree_distributions(snapshots, target_N)

    # Task 2c: Cumulative degree distributions
    plot_cumulative_distributions(snapshots, target_N)

    # Task 2d: Clustering coefficient
    N_values = [100, 200, 500, 1000, 2000, 5000, 10000]
    compute_clustering_coefficient(N_values, m)

#PROGRAMMING ASSIGNMENT 3
import networkx as nx
import numpy as np
import matplotlib.pyplot as plt

# Generate power-law degree sequence
def generate_power_law_sequence(N, gamma):
    degrees = np.random.pareto(gamma - 1, N).astype(int) + 1  # Minimum degree 1
    if sum(degrees) % 2 != 0:
        degrees[np.random.randint(0, N)] += 1
    return degrees

# Simulate attack and track giant component size
def simulate_attack(G, criterion, fractions):
    G_copy = G.copy()
    N = G_copy.number_of_nodes()
    sizes = []

    if criterion == 'degree':
        nodes_sorted = sorted(G_copy.nodes(), key=lambda x: G_copy.degree(x), reverse=True)
    else:  # clustering
        clustering = nx.clustering(G_copy)
        nodes_sorted = sorted(G_copy.nodes(), key=lambda x: clustering[x], reverse=True)

    for f in fractions:
        num_remove = int(f * N)
        G_temp = G_copy.copy()
        G_temp.remove_nodes_from(nodes_sorted[:num_remove])
        if G_temp.number_of_nodes() == 0:
            sizes.append(0)
        else:
            largest_cc = max(nx.connected_components(G_temp), key=len, default=set())
            sizes.append(len(largest_cc) / N)

    return sizes

# Plot giant component sizes
def plot_giant_component(fractions, sizes_degree, sizes_clustering, title):
    plt.figure(figsize=(8, 6))
    plt.plot(fractions, sizes_degree, label='Degree Attack', marker='o')
    plt.plot(fractions, sizes_clustering, label='Clustering Attack', marker='s')
    plt.xlabel('Fraction of Nodes Removed')
    plt.ylabel('Giant Component Size (Normalized)')
    plt.title(title)
    plt.legend()
    plt.grid(True)
    plt.show()

# Main execution
if __name__ == "__main__":
    N = 10000
    fractions = np.linspace(0, 0.5, 11)  # 0 to 50% in 5% steps

    # Task 1: Configuration Model (Power-law, γ = 2.5)
    degrees = generate_power_law_sequence(N, gamma=2.5)
    G_config = nx.configuration_model(degrees)
    G_config = nx.Graph(G_config)  # Convert to simple graph
    sizes_degree_config = simulate_attack(G_config, 'degree', fractions)
    sizes_clustering_config = simulate_attack(G_config, 'clustering', fractions)
    print("Configuration Model (γ = 2.5):")
    print(f"Degree Attack: Giant component sizes = {[f'{s:.3f}' for s in sizes_degree_config]}")
    print(f"Clustering Attack: Giant component sizes = {[f'{s:.3f}' for s in sizes_clustering_config]}")
    plot_giant_component(fractions, sizes_degree_config, sizes_clustering_config,
                        'Giant Component Size vs Fraction Removed (Configuration Model, γ=2.5)')

    # Task 2: Hierarchical Model
    G_hierarchical = nx.powerlaw_cluster_graph(N, m=4, p=0.1)
    sizes_degree_hier = simulate_attack(G_hierarchical, 'degree', fractions)
    sizes_clustering_hier = simulate_attack(G_hierarchical, 'clustering', fractions)
    print("\nHierarchical Model (m=4, p=0.1):")
    print(f"Degree Attack: Giant component sizes = {[f'{s:.3f}' for s in sizes_degree_hier]}")
    print(f"Clustering Attack: Giant component sizes = {[f'{s:.3f}' for s in sizes_clustering_hier]}")
    plot_giant_component(fractions, sizes_degree_hier, sizes_clustering_hier,
                        'Giant Component Size vs Fraction Removed (Hierarchical Model)')

#Programming Assignment 4
import networkx as nx
import numpy as np
import matplotlib.pyplot as plt
from collections import deque

# Generate power-law degree sequence for scale-free network
def generate_scale_free_degree_sequence(N, gamma, avg_degree):
    degrees = np.random.pareto(gamma - 1, N) * N ** (1 / (gamma - 1))
    degrees = degrees.astype(int) + 1  # Ensure positive integers
    current_avg = np.mean(degrees)
    degrees = (degrees * (avg_degree / current_avg)).astype(int)
    if sum(degrees) % 2 != 0:
        degrees[np.random.randint(0, N)] += 1
    return degrees

# Simulate sandpile model
def simulate_sandpile(G, steps=10):
    buckets = {node: G.degree(node) for node in G.nodes()}
    grains = {node: 0 for node in G.nodes()}
    avalanche_sizes = []

    for _ in range(steps):
        node = np.random.choice(list(G.nodes()))
        grains[node] += 1

        unstable = deque([node]) if grains[node] >= buckets[node] else deque()
        current_avalanche_size = 0

        while unstable:
            current_node = unstable.popleft()
            if grains[current_node] >= buckets[current_node]:
                current_avalanche_size += 1
                num_grains = grains[current_node]
                grains[current_node] = 0
                neighbors = list(G.neighbors(current_node))
                if neighbors:
                    grains_per_neighbor = num_grains // len(neighbors)
                    for neighbor in neighbors:
                        grains[neighbor] += grains_per_neighbor
                        if grains[neighbor] >= buckets[neighbor]:
                            unstable.append(neighbor)

        if current_avalanche_size > 0:
            avalanche_sizes.append(current_avalanche_size)

    return avalanche_sizes

# Plot avalanche size distribution
def plot_avalanche_distribution(avalanche_sizes, title):
    plt.figure(figsize=(8, 6))
    plt.hist(avalanche_sizes, bins=30, log=True, density=True)
    plt.xscale('log')
    plt.yscale('log')
    plt.xlabel('Avalanche Size')
    plt.ylabel('Frequency (Log Scale)')
    plt.title(title)
    plt.grid(True, which="both", ls="--")
    plt.show()

# Main execution
if __name__ == "__main__":
    N = 50  # Reduced from 1000
    avg_degree = 2

    # Erdős-Rényi network
    p = avg_degree / (N - 1)
    G_er = nx.erdos_renyi_graph(N, p)
    avalanche_sizes_er = simulate_sandpile(G_er, steps=1000)  # Reduced steps
    print(f"Erdős-Rényi: Mean avalanche size = {np.mean(avalanche_sizes_er):.2f}")
    plot_avalanche_distribution(avalanche_sizes_er,
                              f'Sandpile Avalanche Distribution (Erdős-Rényi, N={N}, <k>={avg_degree})')

    # Scale-free network
    degrees = generate_scale_free_degree_sequence(N, gamma=2.5, avg_degree=avg_degree)
    G_sf = nx.configuration_model(degrees)
    G_sf = nx.Graph(G_sf)  # Convert to simple graph
    avalanche_sizes_sf = simulate_sandpile(G_sf, steps=10)  # Reduced steps
    print(f"Scale-Free: Mean avalanche size = {np.mean(avalanche_sizes_sf):.2f}")
    plot_avalanche_distribution(avalanche_sizes_sf,
                              f'Sandpile Avalanche Distribution (Scale-Free, N={N}, <k>={avg_degree})')

#sample code with GML file

import networkx as nx
import numpy as np
import matplotlib.pyplot as plt

# Function to compute network metrics
def compute_metrics(G, name):
    # Average clustering coefficient
    avg_clustering = nx.average_clustering(G)

    # Average shortest path length (for the largest connected component)
    if nx.is_directed(G):
        G_cc = max(nx.strongly_connected_components(G), key=len)
        G_sub = G.subgraph(G_cc).copy()
    else:
        G_cc = max(nx.connected_components(G), key=len)
        G_sub = G.subgraph(G_cc).copy()
    avg_path_length = nx.average_shortest_path_length(G_sub)

    # Average degree
    degrees = [d for n, d in G.degree()]
    avg_degree = np.mean(degrees)

    print(f"\nMetrics for {name}:")
    print(f"Average Clustering Coefficient: {avg_clustering:.4f}")
    print(f"Average Shortest Path Length: {avg_path_length:.4f}")
    print(f"Average Degree: {avg_degree:.4f}")
    return avg_clustering, avg_path_length, avg_degree, degrees

# Function to visualize network and degree distribution
def visualize_network(G, name, degrees):
    # Create figure with two subplots: network layout and degree distribution
    plt.figure(figsize=(12, 5))

    # Network layout (spring layout)
    plt.subplot(121)
    pos = nx.spring_layout(G, seed=42)  # Consistent layout for reproducibility
    nx.draw(G, pos, node_size=50, node_color='skyblue', edge_color='gray', with_labels=False)
    plt.title(f"{name} Layout")

    # Degree distribution histogram
    plt.subplot(122)
    plt.hist(degrees, bins=20, density=True, color='salmon', edgecolor='black')
    plt.title(f"{name} Degree Distribution")
    plt.xlabel('Degree')
    plt.ylabel('Probability')

    plt.tight_layout()
    plt.show()

# Load the dataset from a GML file
try:
    G_data = nx.read_gml('network.gml')
    print("Loaded network from GML file")
    avg_clust, avg_path, avg_deg, degrees_data = compute_metrics(G_data, "Input Network")
    visualize_network(G_data, "Input Network", degrees_data)
except FileNotFoundError:
    print("GML file not found. Please provide a valid GML file path.")
    # Fallback: Create a sample graph for demonstration
    G_data = nx.karate_club_graph()
    print("Using Karate Club graph as fallback")
    avg_clust, avg_path, avg_deg, degrees_data = compute_metrics(G_data, "Karate Club Network")
    visualize_network(G_data, "Karate Club Network", degrees_data)

# Number of nodes for generated networks
n = G_data.number_of_nodes()
# Approximate number of edges for realistic comparisons
m = G_data.number_of_edges()
# Estimate edge probability for Erdős-Rényi
p = (2 * m) / (n * (n - 1)) if not nx.is_directed(G_data) else m / (n * (n - 1))

# 1. Erdős-Rényi Graph
G_er = nx.erdos_renyi_graph(n, p)
avg_clust_er, avg_path_er, avg_deg_er, degrees_er = compute_metrics(G_er, "Erdős-Rényi Graph")
visualize_network(G_er, "Erdős-Rényi Graph", degrees_er)

# 2. Watts-Strogatz Model
k = int(np.mean([d for n, d in G_data.degree()]))
G_ws = nx.watts_strogatz_graph(n, k, 0.1)
avg_clust_ws, avg_path_ws, avg_deg_ws, degrees_ws = compute_metrics(G_ws, "Watts-Strogatz Graph")
visualize_network(G_ws, "Watts-Strogatz Graph", degrees_ws)

# 3. Scale-Free Network (Barabási-Albert model)
m_ba = max(1, int(m / n)) # Ensure at least 1 edge
G_sf = nx.barabasi_albert_graph(n, m_ba)
avg_clust_sf, avg_path_sf, avg_deg_sf, degrees_sf = compute_metrics(G_sf, "Scale-Free Network")
visualize_network(G_sf, "Scale-Free Network", degrees_sf)

# -*- coding: utf-8 -*-
"""5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-7T7Kyu4LPsiTlCSfdSPyeRZGyAY3TOC
"""

import networkx as nx
import random
import matplotlib.pyplot as plt
import collections

# -*- coding: utf-8 -*-
"""fromgiven.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EgnzqN8Em2TIlcLCabfjM_7mIhG4wqiU
"""

# --- Step 3: Measure Key Properties of Your Graph ---
G_real=nx.read_gml("network.gml")
N = G_real.number_of_nodes()
L = G_real.number_of_edges()
avg_degree = (2 * L) / N
avg_clustering = nx.average_clustering(G_real)

print(f"\n--- Real Graph Properties ---")
print(f"Nodes (N): {N}")
print(f"Edges (L): {L}")
print(f"Average Degree <k>: {avg_degree:.4f}")
print(f"Average Clustering Coefficient C: {avg_clustering:.4f}")

# --- Step 4: Estimate Parameters for Each Model ---
print("\n--- Estimating Model Parameters ---")

# ER Parameters
N_er = N
p_er = avg_degree / (N - 1)
print(f"Erdős-Rényi: N={N_er}, p={p_er:.4f}")

# WS Parameters
N_ws = N
k_ws = round(avg_degree)
if k_ws % 2 != 0: k_ws += 1 # k must be even
print(f"Watts-Strogatz: N={N_ws}, k={k_ws}, p=(requires fitting)")

# SF/BA Parameters
N_sf = N
m_sf = round(avg_degree / 2)
print(f"Scale-Free (BA): N={N_sf}, m={m_sf}")

# --- Step 5: Plot the Real Degree Distribution to Check for Best Fit ---
degree_sequence = sorted([d for n, d in G_real.degree()], reverse=True)
degree_count = collections.Counter(degree_sequence)
deg, cnt = zip(*degree_count.items())

plt.figure(figsize=(8, 6))
plt.loglog(deg, cnt, 'o', color='navy')
plt.title("Degree Distribution of Data from CSV (Log-Log Scale)")
plt.xlabel("Degree (k)")
plt.ylabel("Count P(k)")
plt.grid(True)
plt.show()

import numpy as np
import collections
from scipy import stats

def plot_distribution_fits(G, ax):
    degrees = [G.degree(n) for n in G.nodes()]
    avg_degree = np.mean(degrees)
    num_nodes = G.number_of_nodes()

    degree_counts = collections.Counter(degrees)
    deg, cnt = zip(*degree_counts.items())
    sorted_pairs = sorted(zip(deg, cnt))
    deg, cnt = [list(t) for t in zip(*sorted_pairs)]

    ax.loglog(deg, cnt, 'o', color='b', label='Actual Distribution')
    ax.axvline(avg_degree, color='r', linestyle='--', linewidth=2, label=f'Avg. Degree: {avg_degree:.2f}')

    # --- Power-law fit ---
    log_deg = np.log10(deg)
    log_cnt = np.log10(cnt)
    valid_indices = np.isfinite(log_deg) & np.isfinite(log_cnt)

    if len(log_deg[valid_indices]) > 1:
        slope, intercept, _, _, _ = stats.linregress(log_deg[valid_indices], log_cnt[valid_indices])
        gamma = -slope
        power_law_fit = (10**intercept) * (np.array(deg)**slope)
        ax.loglog(deg, power_law_fit, '--', color='g', label=f'Power-Law Fit (γ ≈ {gamma:.2f})')

    # --- Exponential fit ---
    exp_lambda = 1.0 / avg_degree
    exponential_fit = num_nodes * exp_lambda * np.exp(-exp_lambda * np.array(deg))
    ax.loglog(deg, exponential_fit, '--', color='orange', label='Exponential Fit')

    ax.set_title("Degree Distribution with Fits (Log-Log)")
    ax.set_xlabel("Degree (k) - Log Scale")
    ax.set_ylabel("Count - Log Scale")
    ax.legend()

import networkx as nx
import matplotlib.pyplot as plt

# 1. Create a graph (e.g., Scale-Free)
N = 100
m = 2
G = nx.barabasi_albert_graph(N, m, seed=42)

# 2. Create a figure and axes
fig, ax = plt.subplots(figsize=(8,6))

# 3. Call your function
plot_distribution_fits(G, ax)

# 4. Show the plot
plt.show()

